{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnnaModel.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPnRX/uV8imuLN1wgPFQTP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groupbootcamp2/model/blob/master/AnnaModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RyIoGuS2mtPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3Tlw2mYMjyYH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  \n",
        "  loaded_data = np.load('/content/drive/MyDrive/cfar10_modified_1000.npz')\n",
        "  x_train=loaded_data['train']\n",
        "  x_validation=loaded_data['validation']\n",
        "  x_test=loaded_data['test']\n",
        "  y_train=loaded_data['ytrain']\n",
        "  y_test=loaded_data['ytest']\n",
        "  y_validation=loaded_data['yvalidation']\n",
        "  num_classes =np.max(y_train)+1\n",
        "  # Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "  y_validation = to_categorical(y_validation)\n",
        "  return x_train,y_train,x_validation,y_validation,x_test,y_test"
      ],
      "metadata": {
        "id": "f2Y-CUcQjwvi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prep_pixels(x_train,y_train,x_validation,y_validation,x_test,y_test):\n",
        "  # x_train,y_train,x_validation,y_validation,x_test,y_test=load_dataset()\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_validation = x_validation.astype('float32')\n",
        "\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  x_validation/=255\n",
        "\n",
        "  return x_train,y_train,x_validation,y_validation,x_test,y_test"
      ],
      "metadata": {
        "id": "_WXPg2ybndE0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.3))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(15, activation='softmax'))\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.001, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "VIlcg9jsnqF_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n"
      ],
      "metadata": {
        "id": "dp5pADQKnuXr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\tx_train,y_train,x_validation,y_validation,x_test,y_test = load_dataset()\n",
        "\t# prepare pixel data\n",
        "\tx_train,y_train,x_validation,y_validation,x_test,y_test = prep_pixels(x_train,y_train,x_validation,y_validation,x_test,y_test)\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# create data generator\n",
        "\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "\t# prepare iterator\n",
        "\tit_train = datagen.flow(x_train, y_train, batch_size=32)\n",
        "\t# fit model\n",
        "\tsteps = int(x_train.shape[0] / 64)\n",
        "\thistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=100, validation_data=(x_validation,y_validation), verbose=1)\n",
        "\t# evaluate model\n",
        "\t_, acc = model.evaluate(x_validation,y_validation, verbose=1)\n",
        "\tprint('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        " \n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t06IIWKynzS6",
        "outputId": "e3374dd9-0eb7-489b-9242-5190955a4e70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "703/703 [==============================] - 235s 332ms/step - loss: 2.7158 - accuracy: 0.2063 - val_loss: 2.0146 - val_accuracy: 0.3221\n",
            "469/469 [==============================] - 37s 79ms/step - loss: 2.0146 - accuracy: 0.3221\n",
            "> 32.207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YqVjalPgn69O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}